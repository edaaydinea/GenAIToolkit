{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2936a24c",
   "metadata": {},
   "source": [
    "# Make a Prompt Generating Machine Out of ChatGPT\n",
    "\n",
    "### Summary\n",
    "This video demonstrates a two-step method to transform ChatGPT into a \"prompt generator\" specifically for Large Language Models (LLMs). By first providing a detailed set of instructions on how to act as a prompt generator (including an example structure) and then giving it a target role or title, users can get ChatGPT to create comprehensive, tailored prompts. This technique simplifies advanced prompt engineering, enabling users to quickly generate effective priming prompts for various tasks, such as acting as a Python programming assistant or a marketing expert.\n",
    "\n",
    "### Highlights\n",
    "-   **Two-Step Prompt Generation Process:**\n",
    "    1.  **Initial Setup (Meta-Prompt):** The user provides a detailed prompt to ChatGPT, instructing it to act as a \"prompt generator.\" This initial prompt includes an example of a well-structured target prompt and specifies that ChatGPT should adapt this structure to new titles provided by the user, responding only with \"okay\" to confirm understanding.\n",
    "    2.  **Specific Prompt Request:** Once ChatGPT confirms with \"okay,\" the user provides a concise instruction, typically the desired role for the target prompt, enclosed in quotation marks (e.g., `act as a \"Python programming assistant\"`). ChatGPT then generates a detailed, ready-to-use prompt for that specific role.\n",
    "-   **Meta-Prompting:** This technique is an example of meta-prompting, where an LLM is used to create or refine prompts for itself or other LLMs. It leverages the LLM's understanding of language and context to construct effective instructions.\n",
    "-   **Role-Specific Prompt Creation:** The generator can create tailored prompts for a wide array of roles, such as \"nutrition coach,\" \"Python programming assistant,\" or \"marketing expert for an online shop.\" The generated prompts are designed to be self-explanatory and prime a new LLM instance effectively.\n",
    "-   **Efficiency and Simplification:** This method significantly simplifies the creation of complex prompts. Users only need to define a high-level role, and the pre-programmed ChatGPT instance handles the generation of the detailed prompt structure, saving time and effort.\n",
    "-   **Example-Driven Instruction:** The initial setup prompt uses an example (a prompt for a nutrition coach) to show ChatGPT the desired style, depth, and components of the prompts it should generate, without instructing it to refer to the specific *content* of the example.\n",
    "-   **Using Generated Prompts:** The prompts created by this generator are intended to be copied and used in a new chat session to optimally prime ChatGPT (or another LLM) for the specified task or role.\n",
    "\n",
    "### Conceptual Understanding\n",
    "-   **Meta-Prompting (LLM as a Prompt Generator)**\n",
    "    1.  **Why is this concept important?** Crafting highly effective prompts can be challenging and time-consuming. Meta-prompting allows users to offload some of this cognitive work to the LLM itself, leveraging its capabilities to generate well-structured and nuanced instructions. This is particularly useful for creating complex or specialized prompts without deep prompt engineering expertise.\n",
    "    2.  **How does it connect to real‑world tasks, problems, or applications?** Data scientists or developers can use this to quickly generate base prompts for various tasks like code generation, data analysis summarization, or creating specialized chatbots. For instance, if a team needs many variations of prompts for different customer service scenarios, a prompt generator can create consistent and comprehensive base prompts for each scenario.\n",
    "    3.  **Which related techniques or areas should be studied alongside this concept?** Few-shot prompting (as the initial setup provides an \"example\" of a good prompt), instructional fine-tuning, advanced prompt engineering frameworks, and automated prompt optimization techniques.\n",
    "\n",
    "### Code Examples\n",
    "The video describes a two-step sequence of prompts:\n",
    "\n",
    "**Step 1: Initial Setup Prompt (to make ChatGPT a prompt generator)**\n",
    "```text\n",
    "I want you to act as a prompt generator. First I will give you a title like this one: [Act as a nutrition coach]. Then you provide me with a prompt like this: [I want you to act as a nutrition coach for people who want to improve their eating habits. I will describe my current diet and exercise program, and you will offer suggestions and guidance on how I can improve my overall diet and achieve my health goals. Their responses must focus on dietary recommendations and not include unrelated topics. Provide clear and concise advice without detailed explanations. My first question is what are healthy breakfast options?]. You should adjust the example prompt according to the title I gave you. The prompt should be self-explanatory and appropriate to the title. Do not refer to the example I have given you. Respond exclusively with okay if you understand your role.\n",
    "```\n",
    "\n",
    "**Step 2: Requesting a Specific Prompt (after ChatGPT responds \"okay\")**\n",
    "Provide the desired role within quotation marks. The video shows an example structure for this request:\n",
    "```text\n",
    "My first task is act as a \"Python programming assistant\" (prompt)\n",
    "```\n",
    "*(The user then replaces \"Python programming assistant\" with other roles like \"marketing expert for my online shop\" to get different prompts.)*\n",
    "\n",
    "### Reflective Questions\n",
    "1.  **Application:** Which specific dataset or project could benefit from creating a specialized prompt using this LLM-powered prompt generator? Provide a one-sentence explanation.\n",
    "    -   *Answer: A project aimed at developing a series of educational chatbot tutors for different academic subjects could use this prompt generator to quickly create distinct, comprehensive priming prompts for each subject expert role (e.g., \"Calculus Tutor,\" \"Organic Chemistry Explainer,\" \"Shakespearean Literature Guide\").*\n",
    "2.  **Teaching:** How would you explain the core benefit of using ChatGPT to generate prompts for itself to a junior colleague new to LLMs? Keep the answer under two sentences.\n",
    "    -   *Answer: Instead of you struggling to write the perfect complex instruction for ChatGPT, you teach it once how to write good instructions, and then it does the hard work of crafting those detailed prompts for you for any new task. It's like having an assistant who's also an expert at writing job descriptions for other assistants.*\n",
    "\n",
    "# Search for new Studies about LLMs & Prompt Engineering. Feed it into ChatGPT\n",
    "\n",
    "### Summary\n",
    "This video introduces a technique for leveraging academic research papers, particularly from repositories like arXiv, to enhance prompt engineering with Large Language Models (LLMs) like ChatGPT. By finding relevant papers, extracting their key information (e.g., by copy-pasting text for summarization by ChatGPT), users can stay updated on new prompting concepts, understand the capabilities or limitations of LLMs for specific tasks, and prime ChatGPT with cutting-edge knowledge to formulate more effective prompts.\n",
    "\n",
    "### Highlights\n",
    "-   **Leveraging Research Papers:** Users can find research papers on sites like arXiv.org covering topics such as prompt engineering, LLM capabilities, and new prompting techniques. This allows access to the latest findings in the field.\n",
    "-   **Information Extraction and Summarization:** Instead of reading entire dense papers, the method involves extracting the text (e.g., by copy-pasting from a PDF) and using ChatGPT to provide a quick summary or key points. This makes complex research more accessible.\n",
    "-   **Staying Updated with Prompting Techniques:** By searching for the newest papers on prompt engineering (e.g., \"chain of thought prompting\"), users can quickly get an overview of emerging concepts and potentially ask ChatGPT to help formulate prompts based on these new methods.\n",
    "-   **Informing Prompt Strategy:** The summarized insights from research can directly inform how users prompt LLMs. For instance, a paper might reveal that a specific model or approach is better for a particular task, guiding the user on whether to use a large model like ChatGPT or a smaller, fine-tuned one, as shown in the video's example regarding code-switched machine translation.\n",
    "-   **Priming LLMs with Research Context:** Feeding the summary or key points of a research paper into ChatGPT provides it with specific, up-to-date context. This can be used to then ask more informed questions or request prompts based on the paper's concepts.\n",
    "-   **Practical Workflow:** The suggested workflow is to: 1. Search for a relevant paper on arXiv, 2. Open the PDF, 3. Copy the text (or URL, with the note that direct URL processing will be covered later with plugins), 4. Paste the text into ChatGPT and ask for a summary or key takeaways.\n",
    "\n",
    "### Conceptual Understanding\n",
    "-   **Integrating Academic Research into Practical Prompt Engineering**\n",
    "    1.  **Why is this concept important?** The field of LLMs and prompt engineering is rapidly evolving, with new techniques and insights frequently published in academic papers. Directly incorporating this research allows practitioners to move beyond trial-and-error, applying evidence-based strategies to improve the effectiveness, efficiency, and relevance of their LLM interactions.\n",
    "    2.  **How does it connect to real‑world tasks, problems, or applications?** For data scientists and AI professionals, this means being able to quickly adopt state-of-the-art methods for tasks like complex reasoning, specialized content generation, or understanding LLM biases. For example, if a new paper details a method to improve an LLM's mathematical reasoning, summarizing and applying this can lead to better results when using LLMs for quantitative tasks. It also helps in understanding the limitations and choosing the right model for a job.\n",
    "    3.  **Which related techniques or areas should be studied alongside this concept?** Literature review skills, critical appraisal of research, RAG (Retrieval Augmented Generation) for incorporating external knowledge into LLM responses, and staying updated with AI conferences and journals (e.g., NeurIPS, ICML, ACL).\n",
    "\n",
    "### Code Examples\n",
    "The video does not feature specific programming code but describes a process involving prompts. Here's an example of the type of prompt used after pasting the text from a research paper:\n",
    "\n",
    "```text\n",
    "Please provide a quick summary of the following text. What are the key points of this paper?\n",
    "[Pasted text from the research paper here]\n",
    "```\n",
    "Or, for a more targeted request after ChatGPT has the context:\n",
    "```text\n",
    "Based on the summary of this paper, how would a good prompt look like to apply [specific concept from the paper]?\n",
    "```\n",
    "\n",
    "### Reflective Questions\n",
    "1.  **Application:** Which specific dataset or project could benefit from applying insights gained from a summarized research paper on LLM behavior? Provide a one-sentence explanation.\n",
    "    -   *Answer: A project focused on reducing bias in automated hiring recommendations generated by an LLM could benefit by summarizing the latest research papers on de-biasing techniques for LLMs and then using those insights to structure prompts that explicitly instruct the LLM to avoid gendered or racial language and focus solely on qualifications.*\n",
    "2.  **Teaching:** How would you explain the benefit of using arXiv to find prompt engineering research to a junior colleague who primarily relies on blogs for new techniques? Keep the answer under two sentences.\n",
    "    -   *Answer: arXiv gives you direct access to the newest, often pre-peer-reviewed, research from academics and labs, offering deeper insights and novel techniques before they're widely popularized in blogs, putting you at the cutting edge of prompt engineering.*\n",
    "\n",
    "# SPR for token efficiency if you want to train gpt on a lot of data\n",
    "\n",
    "### Summary\n",
    "This video introduces Sparse Prime Representation (SPR), a technique developed by David Shapiro, designed to manage Large Language Model (LLM) context window limitations by efficiently compressing and decompressing information. By leveraging the LLM's inherent semantic association capabilities, SPR allows users to prime models like ChatGPT with extensive knowledge using significantly fewer tokens through a specialized \"SPR generator\" custom instruction, and then optionally reconstruct the full text for human readability using an \"SPR decompressor\" custom instruction, making it a highly token-efficient method for in-context learning.\n",
    "\n",
    "### Highlights\n",
    "-   **Sparse Prime Representation (SPR):** SPR is a memory organization and in-context learning technique that aims to mimic the compressed and contextually relevant recall patterns of human memory. It allows large amounts of text to be condensed into a \"sparse\" set of \"primes\" (key concepts and associations) that an LLM can understand and use for semantic association.\n",
    "-   **Goal of SPR:** The primary objective is to overcome LLM token limits by representing information in a highly compressed, token-efficient manner. This allows more information to be \"loaded\" into an LLM's context window for tasks like priming or analysis.\n",
    "-   **Semantic Association as Core Principle:** SPR heavily relies on the LLM's ability to perform semantic association. The compressed primes are designed to activate the LLM's latent space (embedded knowledge and abilities) effectively, allowing it to \"understand\" or reconstruct the larger context from these few key points.\n",
    "-   **SPR Generator (Compressor):**\n",
    "    -   **Function:** A custom instruction (system prompt) that configures ChatGPT to act as an \"SPR writer.\" It takes user-provided text and transforms it into a list of concise statements and associations.\n",
    "    -   **Output:** The output is a summary intended for another LLM, not necessarily for direct human comprehension. It uses as few words as possible to capture the conceptual essence of the input.\n",
    "-   **SPR Decompressor:**\n",
    "    -   **Function:** A separate custom instruction that configures ChatGPT to act as an \"SPR decompressor.\" It takes a previously generated SPR as input.\n",
    "    -   **Output:** It reconstructs a full, human-readable article or document by \"unpacking\" the concepts from the SPR, using inference and reasoning to fill in details.\n",
    "-   **Token Efficiency:** SPR is presented as one of the most token-efficient ways to work with LLMs when dealing with large texts. It significantly reduces the number of tokens needed to convey complex information to the model.\n",
    "-   **In-Context Learning:** SPR is a form of in-context learning, where the LLM learns from the information provided directly within the current conversational context, rather than through traditional training or fine-tuning.\n",
    "-   **Custom Instructions:** The technique is implemented by setting specific custom instructions in ChatGPT. These instructions define the LLM's mission (compressor or decompressor), provide the underlying theory (latent space, semantic association), and specify the method to be used.\n",
    "-   **Practical Application:** Users can compress large documents (e.g., research papers, book chapters) into SPRs to prime ChatGPT on the content with minimal token usage. This primed instance can then answer questions or perform tasks based on that knowledge. If needed, the SPR can be decompressed back into a full text.\n",
    "-   **Source:** The SPR concept and the custom instructions are attributed to David Shapiro and are available in his GitHub repository.\n",
    "\n",
    "### Conceptual Understanding\n",
    "-   **Semantic Association and Latent Space Activation**\n",
    "    1.  **Why is this concept important?** LLMs like ChatGPT don't \"understand\" text as humans do but operate by identifying patterns and relationships between words (semantic associations) within a high-dimensional \"latent space\" representing their learned knowledge. SPR leverages this by providing highly potent, condensed cues (primes) that effectively activate relevant portions of this latent space, allowing the LLM to access and utilize a vast amount of related information from very few input tokens.\n",
    "    2.  **How does it connect to real‑world tasks, problems, or applications?** This is crucial for tasks requiring an LLM to process or \"know about\" large volumes of information that exceed its direct input token limit. For example, a data scientist could feed a compressed SPR of an entire research domain to an LLM to then ask nuanced questions or generate hypotheses based on that broad knowledge base, without inputting numerous lengthy papers individually.\n",
    "    3.  **Which related techniques or areas should be studied alongside this concept?** Word embeddings (e.g., Word2Vec, GloVe), attention mechanisms in transformers, knowledge graphs, vector databases, and retrieval-augmented generation (RAG) all touch upon how LLMs represent and access information.\n",
    "\n",
    "-   **In-Context Learning vs. Fine-Tuning**\n",
    "    1.  **Why is this concept important?** SPR is a method for *in-context learning*, meaning the LLM learns \"on the fly\" from the data provided in the prompt. This is distinct from *fine-tuning*, which involves retraining the model's weights on a new dataset and is more resource-intensive. SPR offers a way to adapt the LLM's knowledge for specific tasks quickly and efficiently without altering the base model.\n",
    "    2.  **How does it connect to real‑world tasks, problems, or applications?** For many users, fine-tuning is not feasible due to cost, data requirements, or technical expertise. In-context learning via SPR provides a powerful alternative to imbue the LLM with specific domain knowledge temporarily for a particular session or task, such as summarizing a unique document set or answering questions about proprietary information.\n",
    "    3.  **Which related techniques or areas should be studied alongside this concept?** Few-shot prompting, prompt engineering, meta-learning, and the differences between parametric knowledge (learned during training) and source knowledge (provided in-context).\n",
    "\n",
    "-   **Tokenization and Context Window Limits**\n",
    "    1.  **Why is this concept important?** LLMs process text by breaking it down into tokens (words or sub-word units), and they have a finite limit on how many tokens they can consider at once (the context window). SPR directly addresses this limitation by drastically reducing the number of tokens needed to represent a given piece of information.\n",
    "    2.  **How does it connect to real‑world tasks, problems, or applications?** When dealing with lengthy documents, legal texts, extensive project documentation, or any large corpus, SPR can be the key to making this information \"accessible\" to an LLM within its operational constraints. This enables more comprehensive analysis or interaction than would be possible by simply trying to paste oversized texts.\n",
    "    3.  **Which related techniques or areas should be studied alongside this concept?** Text summarization techniques, data compression algorithms (conceptually), chunking strategies for long texts, and ongoing research into expanding LLM context windows.\n",
    "\n",
    "### Code Examples\n",
    "The video describes setting custom instructions in ChatGPT. These are the system prompts provided by David Shapiro:\n",
    "\n",
    "**1. SPR Generator (Compressor) Custom Instruction:**\n",
    "```text\n",
    "Mission:\n",
    "You are a Sparse Priming Representation (SPR) writer. An SPR is a particular kind of use of language for advanced NLP, NLU, and NLG tasks, particularly useful for the latest generation of Large Language Models (LLMs). You will be given information by the user which you are to render as an SPR.\n",
    "\n",
    "Theory:\n",
    "LLMs are a kind of deep neural network. They have been demonstrated to embed knowledge, abilities, and concepts ranging from reasoning to planning, and even theory of mind. These are called latent abilities and latent content, collectively referred to as latent space. The latent space of an LLM can be activated via the correct series of words as inputs, which will create a useful initial state of the neural network. This is not unlike how the right shorthand cues can prime a human mind to think in certain ways. Like human minds, LLMs are associative, meaning you only need to use the correct associations to prime another model to think in the same way.\n",
    "\n",
    "Method:\n",
    "Render the input as a list of statement associations. The idea is to capture as much conceptually as possible, but with as few words as possible. Write it in a way that makes sense to you, as the further audience will be another Large Language Model, not a human. Use complete sentences.\n",
    "```\n",
    "\n",
    "**2. SPR Decompressor Custom Instruction:**\n",
    "```text\n",
    "Mission:\n",
    "You are a decompressor for Sparse Priming Representations (SPRs).\n",
    "\n",
    "Theory:\n",
    "LLMs are a kind of deep neural network. They have been demonstrated to embed knowledge, abilities, and concepts ranging from reasoning to planning, and even theory of mind. These are called latent abilities and latent content, collectively referred to as latent space. The latent space of an LLM can be activated via the correct series of words as inputs, which will create a useful initial state of the neural network. This is not unlike how the right shorthand cues can prime a human mind to think in certain ways. Like human minds, LLMs are associative.\n",
    "\n",
    "Method:\n",
    "Use the prime-means given to you to fully unpack and articulate the concept. Talk through every aspect, impute what's missing, and use your ability to perform inference and reasoning to fully adjudicate this concept. Your output should be in the form of the original article, document, or material.\n",
    "```\n",
    "\n",
    "### Reflective Questions\n",
    "1.  **Application:** Which specific dataset or project, involving large volumes of text, could significantly benefit from using Sparse Prime Representation for processing with an LLM? Provide a one-sentence explanation.\n",
    "    -   *Answer: A legal tech project requiring LLMs to analyze and summarize thousands of lengthy court case documents could use SPR to compress each document, allowing the LLM to be primed on the core concepts of many cases simultaneously for comparative analysis well within token limits.*\n",
    "2.  **Teaching:** How would you explain the core idea of \"semantic association\" in the context of SPR to a junior colleague who understands basic LLM concepts but not this specific technique? Keep the answer under two sentences.\n",
    "    -   *Answer: Think of SPR as giving the LLM a few very potent keywords or \"primes\"; because the LLM understands how concepts relate (semantic association), these few primes trigger a whole network of related ideas in its \"mind,\" allowing it to grasp the larger picture from a very condensed input.*\n",
    "3.  **Extension:** Beyond compressing existing text, how might the principles of Sparse Prime Representation be adapted to *generate novel content* more efficiently or with greater conceptual density from the outset?\n",
    "    -   *Answer: One could develop a \"Conceptual SPR Planner\" where a user inputs a few core ideas, the LLM expands these into a rich SPR (a dense network of interconnected concepts), and then this SPR is used not to decompress existing text, but as a highly structured and conceptually rich blueprint to generate entirely new, deeply interwoven content, ensuring conceptual coherence and density efficiently.*\n",
    "\n",
    "# Outlook and some Homework\n",
    "\n",
    "### Summary\n",
    "This video provides an outlook on advanced applications of prompt engineering, demonstrating how ChatGPT can be transformed into a \"prompt generating machine\" not only for Large Language Models (LLMs) but also for diffusion models like Midjourney used in image creation. It also previews future course topics such as creating custom GPTs for specialized prompt generation and utilizing browser extensions, while assigning homework to practice with previously taught LLM prompt generators and Sparse Prime Representation for efficient data handling.\n",
    "\n",
    "### Highlights\n",
    "-   **Prompt Generation for Diffusion Models:** The core concept of creating a \"prompt generating machine\" with ChatGPT can be extended to generate detailed and effective prompts for image diffusion models like Midjourney. This involves priming ChatGPT with specific instructions and examples relevant to image generation (e.g., including camera types, lenses, artistic styles).\n",
    "-   **Custom GPTs for Specialized Prompting:** Users can create their own custom GPTs tailored to act as prompt generators for specific purposes, such as generating Midjourney prompts. These custom GPTs are pre-programmed by the user with the necessary instructions and knowledge.\n",
    "-   **Browser Extensions as Prompting Aids:** Tools like Chrome extensions (e.g., AIPRM) can also serve as prompt generating machines or offer libraries of pre-made prompts, simplifying the process for users. This topic will be explored in more detail later in the course.\n",
    "-   **Versatility of Prompt Engineering:** The video emphasizes that the principles of prompt engineering and creating prompt generators are highly versatile and can be adapted for various AI models and tasks, across different modalities like text and image.\n",
    "-   **Call to Practice (Homework):** Viewers are encouraged to practice the techniques learned in previous lessons, specifically:\n",
    "    -   Using the LLM prompt generator to create prompts for text-based tasks.\n",
    "    -   Employing Sparse Prime Representation (SPR) for efficiently priming ChatGPT on large datasets.\n",
    "-   **Future Topics Teased:** The video serves as a bridge to upcoming course content, which will include in-depth lessons on diffusion models, building custom GPTs, and utilizing browser extensions for prompt engineering.\n",
    "\n",
    "### Conceptual Understanding\n",
    "-   **Cross-Modal Application of Prompt Generation Principles**\n",
    "    1.  **Why is this concept important?** The ability to adapt prompt generation strategies from text-based LLMs to other modalities like image diffusion models demonstrates the underlying universality of effective instructional design for AI. It shows that by understanding how to structure requests, provide context, and define desired outputs, one can guide different types of generative AI.\n",
    "    2.  **How does it connect to real‑world tasks, problems, or applications?** This is crucial for creators, marketers, designers, and data scientists who work with multiple forms of AI-generated content. For instance, a marketing campaign might require an LLM to generate ad copy (text) and a diffusion model to create accompanying visuals (images), with both AI systems guided by sophisticated prompts potentially created by a central prompt generating system.\n",
    "    3.  **Which related techniques or areas should be studied alongside this concept?** Multimodal AI, understanding the specific parameters and language of different generative models (e.g., Midjourney's parameter syntax vs. an LLM's natural language instructions), and advanced prompt engineering techniques that can be abstracted across models.\n",
    "\n",
    "-   **Specialized Prompt Generation Tools (Custom GPTs & Extensions)**\n",
    "    1.  **Why is this concept important?** As AI becomes more integrated into workflows, the need for specialized, efficient tools for interacting with AI models increases. Custom GPTs and browser extensions tailored for prompt generation encapsulate complex prompting logic, making advanced AI capabilities accessible to users without requiring them to manually engineer intricate prompts each time.\n",
    "    2.  **How does it connect to real‑world tasks, problems, or applications?** For data science teams, a custom GPT could be built to generate standardized prompts for routine data analysis reports. For content creators, a browser extension might offer quick access to prompts for generating blog post ideas, social media updates, or image concepts, streamlining their workflow significantly.\n",
    "    3.  **Which related techniques or areas should be studied alongside this concept?** API integration for AI models, software development principles for creating tools, user interface (UI) and user experience (UX) design for AI applications, and the development of domain-specific languages (DSLs) for interacting with AI.\n",
    "\n",
    "### Code Examples\n",
    "This video primarily discusses concepts and refers to previously taught methods or future topics. It does not introduce new specific prompts to be copied for immediate use but rather shows the *output* of such systems. The types of prompt generating systems mentioned are:\n",
    "\n",
    "1.  **ChatGPT as a Prompt Generator for Midjourney:** Achieved by providing ChatGPT with initial instructions and examples of Midjourney prompts.\n",
    "    *(Specific setup prompt for this was not detailed in this video but implied to be similar to the LLM prompt generator taught earlier).*\n",
    "2.  **Custom GPTs:** User-created GPT instances programmed to generate specific types of prompts (e.g., for Midjourney).\n",
    "3.  **Browser Extensions (e.g., AIPRM):** Tools that offer interfaces or libraries for generating prompts.\n",
    "\n",
    "The \"homework\" refers to using prompt generators and techniques from *previous* lessons in the course.\n",
    "\n",
    "### Reflective Questions\n",
    "1.  **Application:** Beyond image generation, for which other AI modality or specific AI-powered tool could a custom \"prompt generating machine\" be particularly beneficial? Provide a one-sentence explanation.\n",
    "    -   *Answer: A custom prompt generating machine could be highly beneficial for creating complex musical compositions with AI music generation tools, by translating high-level mood or genre requests into detailed sequences of musical parameters, instrumentation, and structural directives.*\n",
    "2.  **Teaching:** How would you explain the value of creating a custom GPT for prompt generation to a team that currently has each member writing their own prompts from scratch? Keep the answer under two sentences.\n",
    "    -   *Answer: A custom GPT for prompt generation ensures everyone on the team uses consistent, optimized, and high-quality prompts for specific tasks, saving time, reducing variability, and improving the overall quality and predictability of AI outputs.*\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
